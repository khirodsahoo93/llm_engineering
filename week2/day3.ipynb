{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935f1a0-47d1-4c6f-adab-204737fe7503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCE\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "# Please read this! A change from the video:\n",
    "\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which expects to receive `history` in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "But Gradio has been upgraded! Now it will pass in `history` in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function `chat(message, history)` where:  \n",
    "**message** is the prompt to use  \n",
    "**history** is the past conversation, in OpenAI format  \n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'hi'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'hi', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'metadata': None, 'content': 'hi', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'content': 'joke please?'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'hi', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'joke please?', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Sure! Here’s one for you:\\n\\nWhy did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'metadata': None, 'content': 'hi', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'joke please?', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Sure! Here’s one for you:\\n\\nWhy did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', 'options': None}, {'role': 'user', 'content': 'better one'}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7872e077-2a8b-4cbf-99a4-621561236fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat(message):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd682170-3341-4b04-802f-3127ff1e53fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.22.0\n"
     ]
    }
   ],
   "source": [
    "print(gr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9b235f8-01ea-414d-824a-0cc7ee3381c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# -------------------------\n",
    "# Build a Gemini/Perplexity-like shell around your chat(messages) fn\n",
    "# -------------------------\n",
    "def build_chat_ui(chat_fn):\n",
    "    CSS = \"\"\"\n",
    "    body, .gradio-container { background: radial-gradient(1200px 600px at 20% -10%, #c9e7ff55, transparent 60%),\n",
    "                                          radial-gradient(1200px 600px at 90% 10%, #ffd6e955, transparent 60%),\n",
    "                                          linear-gradient(180deg, #0f1115, #0b0d12); }\n",
    "    .topbar { backdrop-filter: blur(10px); background: rgba(255,255,255,0.06); border: 1px solid rgba(255,255,255,0.08);\n",
    "              box-shadow: 0 10px 30px rgba(0,0,0,0.2); border-radius: 20px; padding: 14px 18px; }\n",
    "    .brand { display: flex; align-items: center; gap: 10px; color: #e8eefb; font-weight: 600; }\n",
    "    .brand .dot { width: 10px; height: 10px; border-radius: 999px; background: #8ab4ff; box-shadow: 0 0 12px #8ab4ff; }\n",
    "    .subtitle { color: #9aa5b1; font-size: 0.92rem; }\n",
    "    .wrap { max-width: 1100px; margin: 0 auto; }\n",
    "    .chatpanel, .sidepanel { backdrop-filter: blur(8px); background: rgba(255,255,255,0.04);\n",
    "                              border: 1px solid rgba(255,255,255,0.08); box-shadow: 0 10px 30px rgba(0,0,0,0.25); border-radius: 16px; }\n",
    "    .gr-chat-message { max-width: 780px; margin-left: auto; margin-right: auto; }\n",
    "    .gr-chat-message.user { background: rgba(138, 180, 255, 0.10) !important; border: 1px solid rgba(138, 180, 255, 0.25); }\n",
    "    .gr-chat-message.bot  { background: rgba(255, 255, 255, 0.04) !important; border: 1px solid rgba(255, 255, 255, 0.10); }\n",
    "    .gr-chat-message p, .gr-chat-message li { color: #e9edf6; line-height: 1.65; }\n",
    "    .gr-chat-message code { background: #0b1322; border: 1px solid #14223c; padding: 2px 6px; border-radius: 6px; }\n",
    "    .gr-chat-message pre code { display: block; padding: 12px; border-radius: 10px; }\n",
    "    .gr-textbox textarea { min-height: 56px; font-size: 16px; line-height: 1.35; }\n",
    "    .gr-button { border-radius: 999px; }\n",
    "    .prompt-chip button { width: 100%; text-align: left; background: rgba(255,255,255,0.03);\n",
    "                          border: 1px solid rgba(255,255,255,0.08); border-radius: 12px; }\n",
    "    .prompt-chip button:hover { background: rgba(255,255,255,0.06); }\n",
    "    .footnote { color: #8a94a6; font-size: 12px; text-align: center; padding: 6px 0 2px; }\n",
    "    \"\"\"\n",
    "\n",
    "    suggested_prompts = [\n",
    "        \"Summarize this article in 3 bullets (paste URL):\",\n",
    "        \"Explain like I'm 5: what is vector DB vs RDBMS?\",\n",
    "        \"Draft a polite follow-up email for an interview\",\n",
    "        \"Why did my SQL query get slower after adding an index?\"\n",
    "    ]\n",
    "\n",
    "    # ---- Helpers to keep messages in OpenAI-style schema (type=\"messages\")\n",
    "    def to_msg(role: str, content: str) -> Dict[str, Any]:\n",
    "        return {\"role\": role, \"content\": content}\n",
    "\n",
    "    # Submit: append user -> call chat_fn(messages) -> append assistant\n",
    "    def on_submit(user_text: str,\n",
    "                  messages: List[Dict[str, Any]],\n",
    "                  smart: bool,\n",
    "                  concise: bool):\n",
    "        user_text = (user_text or \"\").strip()\n",
    "        if not user_text:\n",
    "            return gr.update(), messages  # no change\n",
    "        new_msgs = messages + [to_msg(\"user\", user_text)]\n",
    "\n",
    "        # If you want the toggles to reach your model, you can inject system hints here:\n",
    "        sys_hints = []\n",
    "        if smart:\n",
    "            sys_hints.append(\"SmartSearch=on\")\n",
    "        if concise:\n",
    "            sys_hints.append(\"Concise=on\")\n",
    "        if sys_hints:\n",
    "            new_msgs = [to_msg(\"system\", \" ; \".join(sys_hints))] + new_msgs\n",
    "\n",
    "        reply = chat_fn(new_msgs)  # your function\n",
    "        # Normalize reply to string\n",
    "        if isinstance(reply, list):\n",
    "            # If your fn returns messages, take the last assistant string\n",
    "            last_assistant = next((m for m in reversed(reply) if m.get(\"role\") == \"assistant\"), None)\n",
    "            content = last_assistant.get(\"content\") if last_assistant else \"\"\n",
    "        else:\n",
    "            content = str(reply)\n",
    "\n",
    "        new_msgs = messages + [to_msg(\"user\", user_text), to_msg(\"assistant\", content)]\n",
    "        return \"\", new_msgs  # clear textbox, update messages\n",
    "\n",
    "    # Retry: remove last assistant and re-call with same last user\n",
    "    def on_retry(messages: List[Dict[str, Any]]):\n",
    "        if not messages:\n",
    "            return messages\n",
    "        # Remove trailing assistant if present\n",
    "        msgs = messages[:]\n",
    "        if msgs[-1][\"role\"] == \"assistant\":\n",
    "            msgs.pop()\n",
    "        # Find the last contiguous (user ... ) segment to re-ask\n",
    "        # If the last is user, just call once; if assistant removed, last should be user.\n",
    "        if not msgs or msgs[-1][\"role\"] != \"user\":\n",
    "            return messages  # nothing to retry\n",
    "        reply = chat_fn(msgs)\n",
    "        if isinstance(reply, list):\n",
    "            last_assistant = next((m for m in reversed(reply) if m.get(\"role\") == \"assistant\"), None)\n",
    "            content = last_assistant.get(\"content\") if last_assistant else \"\"\n",
    "        else:\n",
    "            content = str(reply)\n",
    "        msgs.append(to_msg(\"assistant\", content))\n",
    "        return msgs\n",
    "\n",
    "    # Undo: pop last message (like Perplexity’s Undo)\n",
    "    def on_undo(messages: List[Dict[str, Any]]):\n",
    "        if not messages:\n",
    "            return messages\n",
    "        return messages[:-1]\n",
    "\n",
    "    # Clear\n",
    "    def on_clear():\n",
    "        return []\n",
    "\n",
    "    with gr.Blocks(css=CSS, fill_height=True, theme=None) as demo:\n",
    "        with gr.Column(elem_classes=[\"wrap\"]):\n",
    "            with gr.Row(elem_classes=[\"topbar\"], equal_height=True):\n",
    "                with gr.Column(scale=6):\n",
    "                    gr.HTML(\n",
    "                        \"\"\"\n",
    "                        <div class=\"brand\">\n",
    "                            <div class=\"dot\"></div>\n",
    "                            <div>Nova Chat</div>\n",
    "                        </div>\n",
    "                        <div class=\"subtitle\">A crisp, focused chat workspace — ask anything.</div>\n",
    "                        \"\"\"\n",
    "                    )\n",
    "                with gr.Column(scale=6, min_width=200):\n",
    "                    web_toggle = gr.Checkbox(value=True, label=\"Smart Search\")\n",
    "                    concise_toggle = gr.Checkbox(value=False, label=\"Concise Mode\")\n",
    "\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=5, elem_classes=[\"chatpanel\"]):\n",
    "                    messages_state = gr.State([])  # list[{\"role\":..., \"content\":...}]\n",
    "                    chatbox = gr.Chatbot(\n",
    "                        height=560,\n",
    "                        type=\"messages\",\n",
    "                        show_copy_button=True,\n",
    "                        label=None,\n",
    "                        value=[]\n",
    "                    )\n",
    "                    textbox = gr.Textbox(\n",
    "                        placeholder=\"Ask anything…  ⏎ to send   |  Shift+⏎ for new line\",\n",
    "                        autofocus=True,\n",
    "                        lines=3,\n",
    "                        submit_btn=gr.Button(\"Send\", variant=\"primary\"),\n",
    "                        show_label=False\n",
    "                    )\n",
    "                    with gr.Row():\n",
    "                        retry_btn = gr.Button(\"↻ Retry\", size=\"sm\")\n",
    "                        undo_btn  = gr.Button(\"↩ Undo\", size=\"sm\")\n",
    "                        clear_btn = gr.Button(\"🧹 Clear\", size=\"sm\")\n",
    "\n",
    "                with gr.Column(scale=2, min_width=260, elem_classes=[\"sidepanel\"]):\n",
    "                    gr.Markdown(\"### Quick prompts\")\n",
    "                    for p in suggested_prompts:\n",
    "                        gr.Button(f\"• {p}\", elem_classes=[\"prompt-chip\"]).click(\n",
    "                            lambda t=p: t, outputs=textbox\n",
    "                        )\n",
    "                    gr.Markdown(\"---\\n### Tips\\n- Toggle **Smart Search** for web-aware replies.\\n- Turn on **Concise Mode** for TL;DR answers.\")\n",
    "                    gr.HTML('<div class=\"footnote\">UI shell inspired by Gemini & Perplexity · Built with Gradio</div>')\n",
    "\n",
    "        # --- Wiring\n",
    "        # Submit flow: textbox -> on_submit -> (textbox empty, messages updated) -> reflect to Chatbot\n",
    "        textbox.submit(\n",
    "            on_submit,\n",
    "            inputs=[textbox, messages_state, web_toggle, concise_toggle],\n",
    "            outputs=[textbox, messages_state],\n",
    "        ).then(\n",
    "            lambda msgs: msgs,  # mirror state to Chatbot\n",
    "            inputs=[messages_state],\n",
    "            outputs=[chatbox],\n",
    "        )\n",
    "\n",
    "        # Retry\n",
    "        retry_btn.click(\n",
    "            on_retry, inputs=[messages_state], outputs=[messages_state]\n",
    "        ).then(\n",
    "            lambda msgs: msgs, inputs=[messages_state], outputs=[chatbox]\n",
    "        )\n",
    "\n",
    "        # Undo\n",
    "        undo_btn.click(\n",
    "            on_undo, inputs=[messages_state], outputs=[messages_state]\n",
    "        ).then(\n",
    "            lambda msgs: msgs, inputs=[messages_state], outputs=[chatbox]\n",
    "        )\n",
    "\n",
    "        # Clear\n",
    "        clear_btn.click(\n",
    "            on_clear, outputs=[messages_state]\n",
    "        ).then(\n",
    "            lambda msgs: msgs, inputs=[messages_state], outputs=[chatbox]\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "# def chat(messages):\n",
    "#     # messages: list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "#     return \"Hello from your model!\"\n",
    "#\n",
    "# demo = build_chat_ui(chat)\n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be6cb95e-6697-485d-adbf-1ff5e86c7b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo=build_chat_ui(simple_chat)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed a bug in this function brilliantly identified by student Gabor M.!\n",
    "# I've also improved the structure of this function\n",
    "\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a57ee0-b945-48a7-a024-01b56a5d4b3e",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business Applications</h2>\n",
    "            <span style=\"color:#181;\">Conversational Assistants are of course a hugely common use case for Gen AI, and the latest frontier models are remarkably good at nuanced conversation. And Gradio makes it easy to have a user interface. Another crucial skill we covered is how to use prompting to provide context, information and examples.\n",
    "<br/><br/>\n",
    "Consider how you could apply an AI Assistant to your business, and make yourself a prototype. Use the system prompt to give context on your business, and set the tone for the LLM.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9e21-df67-4c2b-b952-5e7e7961b03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
